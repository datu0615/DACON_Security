{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_security.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNCbmqyFhWozNp6zIAB5rgr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhctHJ91Cmiv","executionInfo":{"status":"ok","timestamp":1620614801186,"user_tz":-540,"elapsed":675,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"63639e96-91f1-4cbe-d52e-2cba5110a323"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8nmZM6aCnVk","executionInfo":{"status":"ok","timestamp":1620614801432,"user_tz":-540,"elapsed":908,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["path = '/content/drive/MyDrive/security/'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqYfzx39Cwor","executionInfo":{"status":"ok","timestamp":1620614810320,"user_tz":-540,"elapsed":9789,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"8f4f5b48-8067-4dec-8bfb-665cd784cc8b"},"source":["!pip install pytorch-pretrained-bert pytorch-nlp"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n","\u001b[?25hCollecting pytorch-nlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n","\u001b[K     |████████████████████████████████| 92kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/2d/804332ee1eaf36c8be737e9c44da2f2aa449339c220a96b9a15ae7f61443/boto3-1.17.69-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n","\u001b[?25hCollecting botocore<1.21.0,>=1.20.69\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/bb/06710dce8770adf852785785df7e15fc1363596b712766898b1c529e358e/botocore-1.20.69-py2.py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 8.9MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.69->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.69->boto3->pytorch-pretrained-bert) (1.15.0)\n","\u001b[31mERROR: botocore 1.20.69 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\n","Successfully installed boto3-1.17.69 botocore-1.20.69 jmespath-0.10.0 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jlE3rozICs9d","executionInfo":{"status":"ok","timestamp":1620614815001,"user_tz":-540,"elapsed":14463,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["# Import Libraries\n","\n","import tensorflow as tf\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import re\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IktHYWuCzZj","executionInfo":{"status":"ok","timestamp":1620614829754,"user_tz":-540,"elapsed":29209,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["train=pd.read_csv(path+'train.csv')\n","test=pd.read_csv(path+'test.csv')\n","submission=pd.read_csv(path+'sample_submission.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7Hj4CNbKq_h","executionInfo":{"status":"ok","timestamp":1620614865866,"user_tz":-540,"elapsed":65320,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["def alpha_num(text):\n","      return re.sub(r'[^a-zA-z0-9\\s]', '', text)\n","\n","train['full_log'] = train['full_log'].str.lower().apply(alpha_num)\n","test['full_log'] = test['full_log'].str.lower().apply(alpha_num)   "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ogRiJNzKq9K","executionInfo":{"status":"ok","timestamp":1620614888393,"user_tz":-540,"elapsed":87845,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["train['full_log'] = train['full_log'].str.replace(r'[0-9]', '<num>')\n","test['full_log'] = test['full_log'].str.replace(r'[0-9]', '<num>')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"oevVvOEkC1Tj","executionInfo":{"status":"ok","timestamp":1620614888393,"user_tz":-540,"elapsed":87838,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["# Create sentence and label lists\n","sentences = train.full_log.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","labels = train.level.values"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwQcY4aPC5bT","executionInfo":{"status":"ok","timestamp":1620616568791,"user_tz":-540,"elapsed":1768229,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"f9b9e653-da31-4466-e523-045cff7a1b9c"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in tqdm(sentences)]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 5154800.53B/s]\n","100%|██████████| 472972/472972 [28:00<00:00, 281.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Tokenize the first sentence:\n","['[CLS]', 'sep', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', 'local', '##hos', '##t', 'ki', '##bana', 'type', '##er', '##ror', '##time', '##sta', '##mp', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', 't', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', 'z', '##tag', '##s', '[', 'warnings', '##tat', '##sco', '##lle', '##ction', ']', 'pi', '##d', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', 'level', '##er', '##ror', '##er', '##ror', '##mes', '##sa', '##gen', '##o', 'living', 'connections', '##name', '##er', '##ror', '##sta', '##cker', '##ror', 'no', 'living', 'connections', '\\\\', 'n', 'at', 'send', '##re', '##q', '##with', '##con', '##ne', '##ction', 'us', '##rs', '##har', '##eki', '##bana', '##no', '##de', '_', 'modules', '##ela', '##stic', '##sea', '##rch', '##sr', '##cl', '##ib', '##tra', '##ns', '##port', '##js', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '\\\\', 'n', 'at', 'next', 'us', '##rs', '##har', '##eki', '##bana', '##no', '##de', '_', 'modules', '##ela', '##stic', '##sea', '##rch', '##sr', '##cl', '##ib', '##con', '##ne', '##ction', '_', 'pool', '##js', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '\\\\', 'n', 'at', 'process', '_', 'tick', '##cal', '##lb', '##ack', 'internal', '##pro', '##ces', '##s', '##ne', '##xt', '_', 'tick', '##js', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', '<', 'nu', '##m', '>', 'message', '##no', 'living', 'connections', '[SEP]']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wjN50LR-C-7r","executionInfo":{"status":"ok","timestamp":1620616568794,"user_tz":-540,"elapsed":1768224,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}}},"source":["MAX_LEN = 128"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZiuFixkDCj8"},"source":["input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpG4c60uDDxc"},"source":["input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nilww95IDE6z"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jErI5ytHDFxb"},"source":["train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUdmYKKgDHh7"},"source":["train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jM7Qg1YcDJWb"},"source":["batch_size = 32\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkRRY9ALDKXE"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P8_SngowDL8E"},"source":["# Create sentence and label lists\n","sentences = test.full_log.values\n","\n","# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n","sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","\n","MAX_LEN = 128\n","\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","  \n","batch_size = 32  \n","\n","\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3MaEle7GhJb"},"source":["model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n"," # batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyfp3TGPGjTe"},"source":["from sklearn.metrics import matthews_corrcoef\n","matthews_set = []\n","\n","for i in range(len(true_labels)):\n","  matthews = matthews_corrcoef(true_labels[i],\n","                 np.argmax(predictions[i], axis=1).flatten())\n","  matthews_set.append(matthews)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axm9rJAqGk9B"},"source":[""],"execution_count":null,"outputs":[]}]}